# VLM-zero-shot-classification
Vision-Language Models (VLMs) are AI systems that understand and interpret both visual and textual data. By learning from image-text pairs, they enable tasks like image classification, captioning, and visual search. CLIP is a prominent VLM, excelling in zero-shot classification scenarios.
